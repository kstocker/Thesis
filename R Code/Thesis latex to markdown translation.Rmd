---
title: "Statistical Methods in Infectious Disease Modeling"
author: "Kaitlyn Stocker"
date: "4/20/2017"
header-includes:
  - \usepackage{graphicx}
  - \graphicspath{ {/Users/Kaitlyn/Documents/Github/Thesis Git/LaTex Files/Thesis/Plots/} }
  - \usepackage[export]{adjustbox}
  - \usepackage[leftcaption]{sidecap}
  - \usepackage{amssymb}
  - \usepackage[utf8]{inputenc}
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 3
    number_sections: true
    fig_caption: true
bibliography: /Users/Kaitlyn/Documents/Github/Thesis Git/Bibliography.bib
---
\newpage
```{r Libraries, include=FALSE}
library(deSolve)
library(dplyr)
library(rstan)
library(locfit)
library(gridExtra)
library(cowplot)
```

```{r Functions, include=FALSE}
Sim <- function(beta, pop, i.now, s.now, r.now, births) {

results <- as.data.frame(matrix(c(i.now, s.now, r.now), nrow=1))
names(results) <- c("I", "S", "R")
  for(i in 1:545){
    if(s.now*i.now >0){
    n.now <- pop[i]
    i.next <- beta[i]*(i.now^.975)*s.now/n.now
    #i.next <- rnbinom(n=1, mu=beta[i]*(i.now^.975)*s.now/n.now, size=i.now) 
    b.now <- births[i]
    s.now <- s.now - i.next + b.now
    r.now <- r.now + i.now
    i.now <- i.next
    #cases <- i.now/rho[i]
    results <- rbind(results, c(i.now, s.now, r.now))
    }
    else {break}
  }
return(results)
}

read_in_city <- function(city_, naremove){

  df <- measles %>%
  filter(year>1919 & year <1941) %>%
  filter(loc == city_) %>%
  select(biweek, year, cases, pop, rec) %>%
  mutate(number = seq(1, 546, 1))
  
  if(naremove==TRUE){
  for(i in 1:nrow(df)){
    if(is.na(df$cases[i])==TRUE){
      df$cases[i] <- (df$cases[i-1]+df$cases[i+2])/2
    }
  }
  }
  
  for(i in 1:nrow(df)){
    if(is.na(df$cases[i])==FALSE & df$cases[i]==0){
      df$cases[i]<-0.5
    }
  }
  
  betas <- measles_betas %>%
  filter(city == city_) %>%
  select(contains("biweek"))
  
  betas <- unname(unlist(betas[1,]))
  
  return(list(df=df, betas=betas))
} #pass city as character

gaussian_kernel <- function(x) {
  (1/sqrt(2*pi))*exp(-x^2/2)
}

gaussian_sum <- function(i, m) {
  gaussian_sum <- 0
  for(k in 1:m){
      gaussian_sum <- sum(gaussian_kernel((x.n[i,k]-x0[i])/which.diff[i]))
      return(gaussian_sum)
    }
}

gaussian_regression <- function(city, scale){
  cumbirths <- cumsum(city$rec)
  cumcases <- as.numeric(cumsum(city$cases))
  
  ord <- order(cumcases)
  c.cases <- cumcases[ord] 
  c.births <- cumbirths[ord] 

  m <- floor(length(cumcases)*(scale)) #SCALE GOES HERE
  h <- m/2

  x0<-0
  diffs<-matrix(nrow=length(cumcases), ncol=length(cumcases))
  which.diff<-0
  x.n <- matrix(nrow=length(cumcases), ncol=m)
  y.n <- matrix(nrow=length(cumcases), ncol=m)

  for(i in 1:length(cumcases)){
  x0[i] <- c.cases[i] #set focal x 
  diffs[i,] <- abs(c.cases - x0[i]) #difference between each datapoint and x0 (x value difference)
  which.diff[i] <- sort(diffs[i,])[m] #This returns the mth smallest difference between c.births and x0. 

  if(length(c.cases[diffs[i,] <= which.diff[i]])<m){
    x.n[i,] <- c(c.cases[diffs[i,] <= which.diff[i]], rep(0, length.out=(m-length(c.cases[diffs[i,] <= which.diff[i]])))) #select m closest x values to focal x
  y.n[i,] <- c(c.births[diffs[i,] <= which.diff[i]], 0) #select m closest corresponding y valeus 
  }
  else{
  x.n[i,] <- head(c.cases[diffs[i,] <= which.diff[i]], m) #select m closest x values to focal x
  y.n[i,] <- head(c.births[diffs[i,] <= which.diff[i]], m) #select m closest corresponding y valeus 
  }
  }
  

gaussian_kernel <- function(x) {
  (1/sqrt(2*pi))*exp(-x^2/2)
}

gaussian_sum <- function(i, m) {
  gaussian_sum <- 0
  for(k in 1:m){
      gaussian_sum <- sum(gaussian_kernel((x.n[i,k]-x0[i])/which.diff[i]))
      return(gaussian_sum)
    }
}
  
  x.wt.gaus <- matrix(nrow=length(cumcases), ncol=m)
  x.coeffs.gaus <- 0
  for(i in 1:length(cumcases)){
    for(k in 1:m){
  x.wt.gaus[i,k]<- gaussian_kernel(x=(x.n[i,k] - x0[i])/which.diff[i])/gaussian_sum(i,m)
  }
  }
  
  reg.gaus <- lapply(1:length(cumcases), function(x) lm(y.n[x,]~x.n[x,], weights = x.wt.gaus[x,]))

  x.coeffs.gaus <-0
  x.intercept.gaus <-0
  y.fit.gaus <- 0
  for(i in 1:length(reg.gaus)){
    x.coeffs.gaus[i] <- as.numeric(coef(reg.gaus[[i]])[2])
    x.intercept.gaus[i] <- as.numeric(coef(reg.gaus[[i]])[1])
    y.fit.gaus[i] <- x.coeffs.gaus[i]*c.cases[i]+x.intercept.gaus[i]
  }

T <- seq(1, 546, 1)  
x.coeffs.gaus.smooth <- fitted(loess(x.coeffs.gaus ~ T))
y.fit.gaus.smooth <-0
for(i in 1:length(x.coeffs.gaus.smooth)){
  y.fit.gaus.smooth[i] <- x.coeffs.gaus.smooth[i]*c.cases[i] + x.intercept.gaus[i]
}

return(list(y.fit.gaus=y.fit.gaus, y.fit.gaus.smooth=y.fit.gaus.smooth, x.coeffs.gaus=x.coeffs.gaus, x.coeffs.gaus.smooth=x.coeffs.gaus.smooth))
}

y_hat_linear <- function(x,y){
  as.numeric(fitted(lm(y~x)))
}

SSE <- function(y_hat, y){
  SSE<-0
  for(i in seq_along(y)){
  SSE <- sum((y[i]- y_hat[i])^2)
  return(SSE)
  }
}

linear_reconstruction <- function(city){
  cumbirths <- cumsum(city$rec)
  cumcases <- as.numeric(cumsum(city$cases))

  regression_linear <- lm(cumbirths ~ cumcases) 
  Z_linear <- as.vector(resid(regression_linear))
  
  return(list(cumbirths, cumcases, Z_linear))
}

local_regression <- function(city, scale){
  cumbirths <- cumsum(city$rec)
  cumcases <- as.numeric(cumsum(city$cases))
  
  gaus_regression_ <- gaussian_regression(city=city, scale=scale)

  city$Z_gaus <- cumbirths - gaus_regression_$y.fit.gaus
  city$I_gaus <- city$cases*gaus_regression_$x.coeffs.gaus
  
  city$Z_linear <- linear_reconstruction(city)[[3]]
  city$rho <- gaus_regression_$x.coeffs.gaus
  
  return(city)
}

SR_plot <- function(city, name){
    ggplot(city, aes(x=number))+
              geom_line(aes(y=Z_gaus, col="Local Regression Estimate of Z")) +
              geom_line(aes(y=Z_linear, col="Linear Regression Estimate of Z"))+
              labs(x="Biweek", y="Z", title= name)

}

IR_plot <- function(city, name){
  ggplot(city, aes(x=number)) +
              geom_line(aes(y=cases, col="Reported Cases")) +
              geom_line(aes(y=I_gaus, col="Estimate of True Infecteds"))+
              labs(x="Biweek", y="Infecteds/Cases", title = name)
}

rho_plot <- function(city, name){
  ggplot(city, aes(x=number)) +
                geom_line(aes(y=rho, col="rho"))+
                labs(x="biweek", y="estimated rho", title = name)
}

optim_bandwidth <- function(city){
  cumbirths <- cumsum(city$rec)
  cumcases <- as.numeric(cumsum(city$cases))
  
  h<- seq(0.05, 0.89, 0.01)
  SSE1_gaus <- c()
  for(i in seq_along(h)){
    if(SSE(y=cumbirths, y_hat=gaussian_regression(city=city, scale=h[i])[[1]]))
    SSE1_gaus[i] <- SSE(y=cumbirths, y_hat=gaussian_regression(city=city, scale=h[i])[[1]])
  }

  SSE2_gaus <- c()
  for(i in seq_along(h)){
    SSE2_gaus[i] <- SSE(y=y_hat_linear(x=cumcases, y=cumbirths), 
                        y_hat=gaussian_regression(city=city, scale=h[i])[[1]])
  }

  SSE_optim_gaus <- which.min(abs(SSE1_gaus-SSE2_gaus)) #Find where SSE1 and SSE2 intersect

  #Select Optimum scale value
  scale_optim_gaus <- h[SSE_optim_gaus]

  return(scale_optim_gaus)
}

run_bayes <- function(city, which_stan){
  data <- list(N=length(city$number), P=as.integer(city$pop), I=as.integer(city$I_gaus),
              Z=as.integer(city$Z_gaus), Zmin=as.integer(abs(min(city$Z_gaus))))
  
Stan_z_mod <- as.character("
data {
	int<lower=1> N; // time steps
  real P[N]; //population size
	int <lower=0> I[N]; // infecteds
	int  Z[N]; // Zt
  int <lower=0> Zmin;
}

parameters {
  real <lower=Zmin, upper= P[N]> Sbar;
	real<lower=5, upper=100> beta[26];
}

model {
	//priors
  Sbar ~ normal(Zmin, P[1]/10);
	beta[26] ~ gamma(7,5);

	//likelihood
	for (n in 2:N){
  I[n] ~ neg_binomial_2(beta[n%26+1]*(I[n-1]^.975)*(Z[n-1]+Sbar)/P[n-1], I[n-1]);
  }
}    ") 


Stan_z_mod_low_pop <- as.character("
data {
	int<lower=1> N; // time steps
  real P[N]; //population size
	int <lower=0> I[N]; // infecteds
	int  Z[N]; // Zt
  int <lower=0> Zmin;
}

parameters {
  real <lower=Zmin, upper= P[N]> Sbar;
	real<lower=0, upper=200> beta[26];
}

model {
	//priors
  Sbar ~ normal(P[1]/10, P[1]/10);
	beta[26] ~ gamma(10,4);

	//likelihood
	for (n in 2:N){
  I[n] ~ neg_binomial_2(beta[n%26+1]*(I[n-1]^.975)*(Z[n-1]+Sbar)/P[n-1], I[n-1]);
  }
}    ") 

if(which_stan == "low_pop"){
bayes_z <- stan(model_code = Stan_z_mod_low_pop, data=data, iter=500000, chains=4, pars=c("beta", "Sbar"), include=TRUE)
}

if(which_stan == "high_pop"){
  bayes_z <- stan(model_code = Stan_z_mod, data=data, iter=500000, chains=4, pars=c("beta", "Sbar"), include=TRUE)
}

else{bayes_z <- "error: which_stan"}
return(bayes_z)
}

accuracy_check <- function(betas, bayes){
  betas_bayes <- extract(bayes, permuted=TRUE)
  beta_ <- betas_bayes$beta
  Sbar <- betas_bayes$Sbar
  
  beta<- matrix(nrow=1000000, ncol=26)
  for(i in 2:27){
    beta[,i-1] <- beta_[,i%%26+1]
  }
  
  lowlimbeta_sr <- c()
  uplimbeta_sr <- c()
  containbeta_sr <- c()
  for(i in 1:26){lowlimbeta_sr[i] <- quantile(beta[,i], probs=.025, names=FALSE)
                uplimbeta_sr[i] <- quantile(beta[,i], probs=.975, names =FALSE)
                containbeta_sr[i]<- ifelse(lowlimbeta_sr[i] <= betas[i] & uplimbeta_sr[i] 
                                           >=betas[i], 1, 0)
  }


  beta_estimate <- c()
  for(i in 1:26){
    beta_estimate[i] <- quantile(beta[,i], probs=.5, names = FALSE)
  }

  
  lowlim_Sbar <- quantile(Sbar, probs=.025, names=FALSE)
  uplim_Sbar <- quantile(Sbar, probs=.975, names=FALSE)
  
  #take difference of paper values and my estimates
  beta_resid <- beta_estimate - betas
  beta_percent_error <- beta_resid/betas

return(list(containbeta_sr=containbeta_sr, beta_resid=summary(beta_resid), beta_percent_error=summary(beta_percent_error), beta_estimate = beta_estimate, sbar_lower = lowlim_Sbar, sbar_upper = uplim_Sbar))
}

fix_na <- function(df, m, k, j){
  for(i in 1:length(m)){
  df$cases[m[i]] <- (df$cases[k] + df$cases[j])/2
  }
  return(df)
}
```

```{r Read Data, include=FALSE}
#Read in Measles Data from Tycho Paper 
measles <- read.csv(file = "/Users/Kaitlyn/Documents/Thesis Work /Datasets/measlesUKUS.csv", header=TRUE, stringsAsFactors = FALSE)

measles_betas <- read.csv(file = "/Users/Kaitlyn/Documents/Thesis Work /Datasets/measeles biweekly betas.csv", header=TRUE, stringsAsFactors=FALSE)
```

#Introduction 

#Modeling and Inference for Infectious Diseases 

##Deterministic
I began my study of infectious disease modeling by working with a deterministic, continuous time model of an SIR disease. For the sake of simplicity, I began by looking at a closed population in which there was no effect of demographics or migration on the population. 

When modeling the spread of an infectious disease through a population, there are two key parameters of interest: the transmission rate $\beta$, and the recovery rate $\gamma$. The transmission rate is the product of the rate of contact between susceptibles and infecteds in a given population, and the duration of the infection is the average length of time an individual will remain in the infected class. The duration of the infection is then given by the reciprocal of the recovery rate, $frac{1}{\gamma}$. These parameters, together with the initial values of S, I, and R, are the necessary pieces of information required to simulate the spread of the infection through a population. 

The proportion of susceptibles, infecteds, and recovered individuals over time is represented by a series of differential equations given by the following:

\begin{equation}\frac{dS}{dt} = -\beta SI \end{equation}
\begin{equation}\frac{dI}{dt} = \beta SI - \gamma I \end{equation}
\begin{equation}\frac{dR}{dt} = \gamma I \end{equation}

In this model, $\beta SI$ is the transmission term, and represents the number of individuals flowing from the susceptible class into the infected class, and $\gamma I$ represents the flow of individuals from the infected class into the recovered class. 

\paragraph{Simulation}
To produce an example of what this model looks like in action, I simulated the spread of influenza through a boy's boarding school. I took the values of the parameters and the initial conditions from the book [@KeelingBook]. In this example, $\beta$ is 1.66, and $\gamma$ is $\frac{1}{2.2}$. In a population of 763 boys, at the start of the epidemic 3 were infected and the rest were in the susceptible class. 

As the differential equations defining the model are not possible to solve explicitly, I used Euler's method to solve the system. In R, I ran the system of equations through Euler's method with a 0.01 time step for a period of 15 days. I outputted a data frame that included the value of S, I, and R for each time step through completion. Figure \ref{DeterministicSim} shows a plot of the proportion of each infection class over time. 
 
 
```{r DeterministicSim, echo=FALSE, fig.cap="Deterministic SIR Simulation \\label{DeterministicSim}", fig.width=4, fig.height=4}
SIRsim <- function (parameters = NULL, initial = NULL, time = NULL, ...) ## parameters should be a vector of beta and gamman, initial should be a vector of S(o), T(o), R(o), and time should be start time, end time, dt.
{
  evalODE <- function(parameters = NULL, initial = NULL, time = NULL) {
    diff_eqs <- function(time, initial, parameters) { # diff equations for SIR
      with(as.list(c(initial, parameters)), {
        dS <- -beta * S * I
        dI <- beta * S * I - gamma * I
        dR <- gamma * I
        list(c(dS, dI, dR)) # output them as a list
      })
    }
    
    eval <- ode(times = time, func = diff_eqs, y = initial, # solve the ode using euler method
                  parms = parameters, method = "euler", ...)
    return(eval) # evalODE returns the solved ODE's given the parameters/conditions
  }
  output <- evalODE(parameters,initial,time) # save the output of evalODE
  return(list(model = evalODE, pars = parameters, init = initial, 
              time = time, results = as.data.frame(output))) # return as a list with other columns
}

plotSIR <- function(output) { # output should be a dataframe (or list) output from SIRsim
  ggplot(output$results, aes(time)) + 
    geom_line(aes(y = S, color = "S")) + 
    geom_line(aes(y = I, color = "I")) +
    geom_line(aes(y = R, color = "R")) +
    labs(x="Time (days)", y= "Proportion of Population") +
    ggtitle("Influenza at a Boarding School Simulation")
}


### Simulating
parameters <- c(beta = 1.66, gamma = (1/2.2)) # Beta, Gamma
initial <- c(S = (760/763), I = (3/763), R = 0) # S(o), I(o), R(o)
time <- seq(0,15,.1) # T0 to T by dt
influenzaTest <- SIRsim(parameters,initial,time)

## Plotting
plotSIR(influenzaTest)
```


\paragraph{Inference}
I then ran inference on the simulated data to retrieve back the value of $\beta$. To do this, I first created a function that simulated data for a series of $\beta$ values ranging from 0 to 3 with a step of 0.01, maintaining the same initial conditions and $\gamma$ value as the initial simulation. This function created a data frame of results for each value of $\beta$. I then ran a sum of squares function and took the squared difference between the results of my original simulation and the results of my estimation function. A plot of the estimated $\beta$ values against the resulting sum of squares output is presented in figure \ref{SIRndEST}. It is visually evident that the minimum of the function occurs at 1.66, the actual value of $\beta$ that I used to simulate my data. Running the optim function in R to minimize the sum of squares function returned the expected $\beta$ of 1.66. 

```{r SIRndEST, echo=FALSE, fig.cap="Plot of estimated beta values against their sum of squares output \\label{SIRndEST}"}

estimateBeta <- function(data, beta, gamma, initial, time) {
  j = 0
  i = 1
  emptyMatrix = matrix(,nrow=((3/.05)+1),ncol=2) # create empty matrix of the correct size
  tempVar = beta # set initial beta equal to a temp variable
  
  while (beta <= (tempVar + 3)) { # loop through betas until you get to initial beta plus 3
    SSBeta = 0 # (re)initialize SS to be zero
    parameters <- c(beta = beta, gamma = gamma) # save new beta as it goes through the loop
    sim <- SIRsim(parameters,initial,time) # solve the ODE's given that beta
    for (j in seq(1,nrow(data$results))) { # loop through each t to compute the residuals
      SSBeta = SSBeta + (sim$results$I[j] - data$results$I[j])^2
    }
    emptyMatrix[i,] <- c(beta, SSBeta) # put beta and SSbeta into the matrix
    i = i + 1 # update row of matrix
    j = 0 # reset j to be zero
    beta = beta + .05 # .05 is arbitrary, will probably make it into a parameter of the function so it can be changed easily
  }
  colnames(emptyMatrix) <- c("Beta", "SSBeta") # change the col names
  return(emptyMatrix)
}

graphBetaEstimates <- function(matrix) { # takes matrix of betas and SSBeta
  tempDF <- as.data.frame(betaMatrix)
  ggplot(tempDF, aes(Beta)) + 
    geom_line(aes(y = SSBeta, color = "SSBeta")) + 
    labs(x="Beta", y= "Sum of Squares") +
    ggtitle("Beta Estimation for SIR Simulated Data")+
    scale_x_continuous(breaks = scales::pretty_breaks(n = 20))  # set x-scale to be something smaller than default
}

  betaMatrix <- estimateBeta(influenzaTest, 0, (1/2.2), initial, time)
graphBetaEstimates(betaMatrix)
```


##Stochastic

###Chain Binomial
One way of adding stochasticity is to use a chain binomial model. In this type of model, we allow the the number of infecteds at each time step to follow a random binomial distribution in which the number of trials is equal to the number of susceptibles at the previous time step (the pool of individuals who could potentially be infected). If we consider \textit{p} to be the probability that contact occurs between a susceptible and a single infected individual and that the contact results in the infection, then the probability of an individual escaping infection from one infected is given by (1-\textit{p}). In order for a given susceptible to escape infection entirely, they must avoid infection from all infecteds in the population at that time. In this way, the probability of a susceptible escaping infection entirely is given by $(1-p)^{I}$. In this case, the probability that a given susceptible will become infected in a given time step is given by $1-(1-p)^{I}$. It follows that the number of infecteds in a given time step follows a binomial distribution with $S_{t-1}$ as the number of trials and $1-(1-p)^{I}$ as the probability of success. 

The next logical step is to define \textit{p} in terms of our parameters, $\beta$ and $\gamma$. We can do this by defining the probability that a given susceptible will become infected in a given time step (with $I_{t}$ infecteds) as $1-exp(-\beta I_{t}/N)$, where \textit{N} is the total population size. I divided the number of infecteds by the population size to obtain the proportion of infecteds in the population. This was necessary because $\beta$ is derived for use with population proportions, and in this model S, I, and R will refer to the number of individuals in each class. We allow length of the infectious period, $\gamma$, to be equal to the time step. In this way, at the end of each time step the infecteds from the previous time step all move into the recovered class. 

\paragraph{Simulation}

I simulated an SIR infection using the chain binomial model. For consistency and comparison, I used the parameters and starting conditions from the Influenza at a Boarding School example, the same example that I used to simulate the deterministic example, taken from [@KeelingBook]. 


```{r ChainBinomSim, echo=FALSE, fig.cap="Chain Binomial Simulation \\label{ChainBinomSim}"}
Sim <- function(beta, n, i.now, s.now, r.now, time.steps) {

results <- as.data.frame(matrix(c(0, i.now, s.now, r.now), nrow=1))
names(results) <- c("time", "I", "S", "R")

  for(j in 1:time.steps){
    i.next <- rbinom(1, s.now, 1-exp(-beta*i.now/n))
    s.now <- s.now - i.next
    r.now <- r.now + i.now
    i.now <- i.next
    results <- rbind(results, c(j, i.now, s.now, r.now))
  }
return(results)
}


#PLOT
plot_sim <- function(results){
  ggplot(results, aes(x=time)) +
  geom_line(aes(y=S, col="S"))+
  geom_line(aes(y=I, col="I"))+
  geom_line(aes(y=R, col="R"))+
  ggtitle("Chain Binomial SIR Simulation")+
  labs(x="Time Steps", y="Number of Individuals")
}

#Simulating
i.now <- 3
s.now <- 760
r.now <- 0
time.steps <- 25
beta <- 1.66*2.2
n <- 763

results <- Sim(beta=beta, n=n, i.now=i.now, s.now=s.now, r.now=r.now, time.steps)


#Plotting Simulation
plot_sim(results)
```


To achieve the output in figure \ref{ChainBinomSim}, I ran equations (4) through (6) through a loop for 25 time steps. I chose 25 time steps based on the length of the epidemic observed from running the example using a deterministic model. 

\begin{equation} 
I_{t+1} \sim Binomial(S_{t}, 1-exp(\frac{-\beta I}{N})) 
\end{equation}
\begin{equation} 
S_{t+1} = S_{t} - I_{t+1} 
\end{equation}
\begin{equation}
R_{t+1} = R_{t} + I_{t}
\end{equation}

It is clear from figure \ref{ChainBinomSim} and figure \ref{SIRndSim} that the chain binomial model does not have as dramatic of a peak as the deterministic model. 


\paragraph{MLE Inference:}
To run inference on the chain binomial model, I used a maximum likelihood estimate (MLE) approach. To do this, I first found the likelihood function for $\beta$, which is given by equations (7) and (8) below. 


\begin{equation}
\mathcal{L}(\beta) =  \prod_{i=1}^n {I_{i-1}\choose S_{I-1}} p^{I_{i}}(1-p)^{S_{i}}
~, ~where ~ p = 1-exp(\frac{-\beta I_{i-1}}{N})
\end{equation}
This can be simplified to:
\begin{equation}
\mathcal{L}(\beta) \propto p^{I_{[i]}}(1-p)^{S_{[i]}}
\end{equation}

I then used the optimize function in R to compute the value of $\beta$ that maximized the log likelihood function, and received a value of 3.65, which is equal to the value I simulated with, $\beta=1.66*2.2=3.65$. A plot of the likelihood function can be found in figure \ref{ChainBinomMLE}.


```{r ChainBinomMLE, echo=FALSE, fig.cap="Plot of the likelihood function for beta for the chain binomial model \\label{ChainBimonMLE}"}
MLE <- function(beta1, results){
likelihood <- function(beta1, results) {
  z <- 0
  for(j in 2:time.steps){
    p <- (1-exp(-beta1*results$I[j-1]/n)) #probability of infection 
    z <- z - log(p^results$I[j] * (1-p)^results$S[j]) #negative log likelihood
  }
  return(z)
}
#MLE <- optim(par=beta1, fn=likelihood, method="Brent", lower=0, upper=3, results=results)
MLE <- optimise(likelihood, lower=0, upper=5, results=results)
return(MLE)
}

likelihood <- function(beta1, results) {
  z <- 0
  out <- c(z)
  for(j in 2:time.steps){
    p <- (1-exp(-beta1*results$I[j-1]/n)) #probability of infection 
    z <- z - log(p^results$I[j] * (1-p)^results$S[j]) #negative log likelihood
    out <- cbind(out, z)
  }
  out<-as.vector(out)
  return(out)
}

plot_likelihood <- function(likelihood){
  ggplot(, aes(x=seq(1, 5, length.out=length(likelihood))))+
    geom_line(aes(y=likelihood))
}

beta1 <- 1
estimate <- MLE(beta1=beta1, results)
likelihood1 <- likelihood(beta1=beta1, results)
plot_likelihood(likelihood1)

```


\paragraph{Bayesian Inference:}


Unlike the classical approach, which treats model parameters as fixed values, Bayesian inference treats model parameters as random variables. The distribution of the parameters is calculated via Bayes' Theorem based on information given via a prior distribution and a likelihood computed based on the data. This final distribution is called the posterior distribution, and it gives all relevant information about the parameters, including point and interval estimates. The posterior distribution is defined more precisely in the equation below:



\begin{equation}
P(\theta \mid y) = \frac{P(\theta)P(y \mid \theta)}{\int P(\theta)P(y \mid \theta) d\theta}
\end{equation}



Where $P(\theta)$ is the prior distribution, $y$ is the data, and $P(y \mid \theta)$ is the likelihood function. 

To run inference on my simulated epidemic data, I used Bayesian inference with the Markov chain Monte Carlo (MCMC) method. The MCMC method allows samples to be drawn from the target distribution - in this case, the samples are drawn from the joint posterior distribution of the model parameter. 

I started by choosing a prior distribution for $\beta$ and $\gamma$. I chose a gamma distribution with shape parameter equal to 3 and scale parameter equal to 1. This distribution has the majority of its density between 0.5 and 5, which is a reasonable range within which to expect $\beta$. For the prior on $\gamma$ I chose a gamma distribution with shape parameter equal to 2 and a scale parameter equal to 1. I chose this prior for $\gamma$ because it has a strong right skew, and $\gamma$ is typically less than one, as it is the reciprocal of the infectious period. 

I used the package RStan to run Bayesian inference with the MCMC method on my simulated chain binomial model, with the aforementioned priors. Figure \ref{ChainBinomBayesDensity} displays the posterior density plots of $\beta$ and $\gamma$. 


The outputted estimate for $\beta$ was 1.66, with a standard error of 0. Recall that the true value of $\beta$ is 1.66. This level of accuracy is possible only with data simulated without any noise. Figure \ref{ChainBinomPosPriorPlots} also shows the lack of error in the posterior density of $\beta$. The outputted estimate for $\gamma$ was 0.76 with a standard error of 0.33. The 95\% credible interval for $\gamma$ was (0.13, 1.40), which contains the true $\gamma$ of $\frac{1}{2.2}$ or 0.455. 

Based on simulated examples, recovery of $\beta$ appears to be more precise than that of $\gamma$. This may, however, be a result of simulation mechanics. For small population sizes, recovery of $\gamma$ was unreliable. However, population size did not affect the ability to accurately recover $\beta$.  


###TSIR


####Temporally Varying Transmission Rates
Up to this point, all of my examples have worked under the assumption that $\beta$, the transmission rate of the infection, is constant across time. However, this is really not the case! Many diseases display temporal shifts in transmission rate. For instance, most childhood diseases (such as measles or chickenpox) have higher transmission rates during school terms, and low transmission rates during school breaks. This makes intuitive sense - I would expect an infected child to come in contact with more susceptible children during school terms, when they are in constant contact with other children, than during school breaks, when they may be more isolated from their susceptible peers. 

Measles during the pre-vaccination era displays temporally varying transmission rates, and I will be focusing from here on out on the dynamics of the measles virus. Measles has an infectious period of 2 weeks, and it is commonly modeled as having 26 time-varying transmission rates (one for each bi-week of the year). 

It follows that the transmission rate can be expressed as a function of time, $\beta(t)$. Researcher Bailey (1975) found the transmission rate to be the sinusoid function given in equation 19 below:

\begin{equation}
\beta(t) = \beta_{0}(1+ \beta_{1}cos(\omega t))
\end{equation}

The parameter $\beta_{0}$ in equation 19 gives the baseline transmission rate. The parameter $\omega$ gives the period of seasonal forcing, and is therefore equal to $\frac{\pi}{T}$ where $T$ is the number of $\beta$'s in a single calendar year. The parameter $\beta_{1}$ determines the amplitude of the seasonality and is therefore bounded between 0 and 1. For measles, $\beta_{0}$ is around 17 new cases per biweek, and $\omega = \frac{\pi}{26}$. In the case of seasonal transmission rates, $R_{0} = \frac{\beta_{0}}{\gamma}$.


###Susceptible Reconstruction
In previous exercises, I had access to perfect and complete simulated data. While having such complete data is convenient, it is not realistic. Data gathered in real-world situations is much less inference-ready than the simulated data I have been using thus far.

Real data deviates from simulated data in a number of significant ways. For one thing, real data is incomplete. Not all cases of a disease are reported, so the number of infecteds at any given time point must be estimated using the number of reported cases multiplied by the reporting rate. There is typically no information about the true number of susceptible or recovered individuals, as collecting this information would be extremely impractical and cost-prohibitive. 

In order to run any kind of meaningful inference on epidemic data, it is necessary to have at minimum the infected and susceptible dynamics over time. Using the reported cases and the rate at which cases are reported, it is easy enough to construct the infected class dynamics. However, reconstructing the susceptible class dynamics is not so straight-forward. 

In order to reconstruct the susceptible class dynamics, let's first define the model. I will continue with the basic SIR model, but this time I am going to add in birth dynamics. The addition of birth dynamics into the susceptible class are crucial to the susceptible reconstruction process. To do this, I will define $B_{t-d}$ as the number of births at time $t-d$. Since infants are born with natural immunity from their mothers, there is a time delay (denoted by $d$) between when a baby is born and when it enters the susceptible class. The length of this delay is dependent on the disease. As before, I define the size of the infected class at a given time point $t$ to be $I_{t}, \in{\{1,...,T\}}$. Similarly, I define the size of the susceptible class at a given time point $t$ to be $S_{t} \in{\{1,...,T\}}$. Equations 12 and 13 give the model specifications. 

\begin{equation}
I_{t} = \beta S_{t-1} I_{t-1}
\end{equation}
\begin{equation}
S_{t} = B_{t-d} + S_{t-1} - I_{t}
\end{equation}

In equation 14 I allow $I_{t}$ to be a product of the number of reported cases, $C_{t}$ and $\rho_{t}$, the reporting rate at time $t$. I define $\rho$ such that when $\rho_{t} =1$, the number of true cases has been fully reported. When $\rho_{t} > 1$, the number of true cases has been underreported. Additionally, I assume that $\rho_{t}$ follows a probability distribution with  $E(\rho_{t}) = \rho$.

\begin{equation}
I_{t} = \rho_{t}C_{t}
\end{equation}

Substituting equation 14 into equation 13, we get: 

\begin{equation}
S_{t} = B_{t-d} + S_{t-1} -  \rho_{t} C_{t}
\end{equation}

If we define $E(S_{t})=\bar{S}$, then we can define a new variable $Z_{t}$ such that $S_{t} = \bar{S} + Z_{t}$, with $E(Z_{t})=0$. In this way, $Z_{t}$ is the deviations from the mean of $S_{t}$. $Z_{t}$ therefore follows the same recursive relationship as $S_{t}$, and can be defined as follows:

\begin{equation}
Z_{t} = B_{t-d} + Z_{t-1} -  \rho_{t} C_{t}
\end{equation}

If we allow $Z_{0}$ to be the initial value of Z, we can rewrite the previous equation to look like the following:

\begin{equation}
Z_{t} = Z_{0} + \sum_{i=1}^{t} B_{i-d} - \sum_{i=1}^{t} \rho_{i}C_{i}
\end{equation}

To de-clutter this notation, allow $Y_{t}=\sum_{i=1}^{t} B_{i-d}$ and $X_{t}= \sum_{i=1}^{t}C_{i}$. Additionally, we will assume a constant reporting rate. Now we can rewrite equation 17 as a simple linear regression equation:

\begin{equation}
Y_{t} = -Z_{0} + Z_{t} + \rho X_{t}
\end{equation}

Thus we have a linear regression equation relating cumulative births ($Y_{t}$) to cumulative reported cases ($X_{t}$). The susceptible dynamics $Z_{t}$ are the regression remainder to equation 18, and can thus be fully reconstructed. 

The infected dynamics are reconstructed by multiplying the reported cases at each time step by $\rho$, which is obtained as the slope of the linear regression equation. 

####Non-Constant Reporting Rate
In my previous explanation of susceptible reconstruction, I made a crucial assumption about the rate at which infected individuals are reported: I assumed that the reporting rate was constant across time. This assumption simplified the process of reconstructing susceptible and infected dynamics, but it rarely holds true in the world of real data. 

As an example, let's look at measles data from pre-vaccination era New York City,  from 1920-1940 [@tycho]. In figure \ref{Zlocal}, plot (A) is a graph of the reconstructed susceptible dynamics, obtained as previously described using global linear regression. It is evident from plot (A) in figure \ref{Zlocal} that the Z dynamics suffer from local shifts away from the mean of zero. This indicates that the previously held assumption of constant reporting rate, $\rho$, has been violated. 



```{r Zlinear, echo=FALSE, fig.cap="Plot of the reconstructed Z parameter for measles data in New York City from 1920-1940 using global linear regression, as described in the susceptible reconstruction section above. \\label{Zlinear}"}
```


When local shifts in the mean of the Z dynamics are observed, such as those in plot (A) from Figure \ref{Zlocal}, it indicates a nonconstant reporting rate. Following the work of Finkenstadt and Grenfell [@SRgrenfell], I addressed the issue of time-varying reporting rate by performing local linear regression with gaussian smoothers. To give an overview before I break down the process in detail, I split the data into overlapping chunks (or neighborhoods) centered around each $\{X_{i},...,X_{N}\}$, then assigned a gaussian weight to each observation in each neighborhood. Then I performed $N$ weighted linear regressions using the previously defined gaussian weights, and derived the value of $\rho$ for each time point by pulling the slope from each of these weighted linear regressions. 

To break it down, I began as I did in my previous Susceptible Reconstruction example by computing the cumulative cases, $X_{t}= \sum_{i=1}^{t}C_{i}$, and the cumulative births, $Y_{t}=\sum_{i=1}^{t} B_{i-d}$. I then split the data into neighborhoods. To do this, I defined a bandwidth, $h$ and a neighborhood size $m=T*h$ (where $T$ is the number of observations) such that for each $\{x_{t},...x_{T}\}$, a neighborhood was constructed consisting of the $m$ closest datapoints to the value of $x_{t}$. In this way, I constructed a matrix $X_{n}$ with $T$ rows and $m$ columns. I additionally constructed a corresponding $Y_{n}$ matrix. 

I chose the bandwidth, $h$, in accordance with the method outlined by Finkenstadt and Grenfell [@SRgrenfell]. As Finkenstadt and Grenfell argued, automatic selection processes that minimize the residual to white noise are not suitable, as the residuals in the regression are significant. The method they proposed was to choose the bandwidth that minimized the difference between two sums of squares: the sum of square errors ($SSE_{1}(h)$), and the sum of squares of the deviations of the local estimator from the linear estimator ($SSE_{2}(h)$), where $h$ is the bandwidth using a guassian kernel. Essentially, their method chooses a bandwidth that both minimizes the SSE while preventing over smoothing by tethering the local regression to the estimator obtained from the global linear regression. In the equations that follow, $\hat{m}_{h,t}(x_{t})$ is the local estimator at point $x$ with smothing parameter $h$. 

\begin{equation}
SSE_{1}(h) = \sum_{t=1}^{T}\{Y_{t}-\hat{m}_{h,t}(x_{t})\}^2
\end{equation}
\begin{equation}
SSE_{2}(h) = \sum_{t=1}^{T}\{\hat{Y}_{t}-\hat{m}_{h,t}(x_{t})\}^2
\end{equation}

In figure \ref{Zlocal} I reference data from the Tycho database [@tycho] for pre-vaccination measeles in New York City. The bandwidth that minimized the difference between $SSE_{1}$ and $SSE_{2}$ for this data was $h=0.28$. 

The next step was to apply a weight function. Following the work of Finkenstadt and Grenfell [@SRgrenfell], I used a gaussian weight function, defined by the following:

\begin{equation}
K(x)=\frac{1}{\sqrt{2*pi}}e^{\frac{-x^{2}}{2}}
\end{equation}
\begin{equation}
w_{i}(x_{t})=\frac{K(\frac{x_{0}-x_{i}}{h})}{\sum_{i=1}^{m}K(\frac{x_{0}-x_{i}}{h})}
\end{equation} 
<!-- note that liberties were taken in defining weight function - I added the subscript 0 for the x's. Notation could definitely use work --> 

In which $K(x)$ is the gaussian kernel function, and $w_{i}(x)$ gives the weight function. I applied the weight function to each row of the $X_{n}$ matrix, for which $x_{0}$ is the focal, or central, $x$ value. 

To pull this together with an example, if I were to look at the datapoint $x_{10}$ (the focal $x_t$ of the $10^{th}$ row of the $X_{n}$ matrix), I would first create a vector of length $m$ of the $x_{t}$'s nearest to $x_{10}$. I would then apply the weight function $w_{i}(x_{10})=\frac{K(\frac{x_{10}-x_{i}}{h})}{\sum_{i=1}^{m}K(\frac{x_{10}-x_{i}}{h})}$ to each of the $m$ $x_{t}$'s in the neighborhood of $x_{10}$. I repeated this for each row of the $X_{n}$ matrix, resulting in a weight matrix with the same dimensions of $X_{n}$. 

Once I computed the gaussian weight for each datapoint in the $X_{n}$ matrix, I ran a weighted linear regression on each of the $T$ rows of the $X_{n}$ matrix, using the gaussian weights from the previously computed weight matrix. The result of this was $T$ simple linear regression outputs. It follows that $\rho_{t}$ is a vector of the slopes of these linear regressions. I obtained fitted values by imputting the focal x ($x_{0}$) into the linear model for each time point. I then took the residual of the regression and obtained the $Z$ dynamics from the local regression. 


```{r LocalReg, echo=FALSE, fig.cap="Comparison of reconstructed Z dynamics from the New York data. Plot (A) shows Z dynamics obtained via global linear regression. Plot (B) shows Z dynamics obtained via local linear regression. \\label{Zlocal}"}
newyork <- read_in_city("NEW YORK", naremove=FALSE)[[1]]

scale_optim_ny <- 0.28 #from previous analysis 

newyork <- local_regression(newyork, scale_optim_ny)

linearplot <- ggplot(newyork, aes(x=number))+
                geom_line(aes(y=newyork$Z_linear))+
                labs(x="Biweek", y="Z")+
                geom_hline(aes(yintercept=0), linetype=3)

localplot <- ggplot(newyork, aes(x=number))+
                geom_line(aes(y=newyork$Z_gaus))+
                labs(x="Biweek", y="Z")+
                geom_hline(aes(yintercept=0), linetype=3)

plot_grid(linearplot, localplot, labels=c("A", "B"), ncol = 1, nrow = 2)
```


Figure \ref{Zlocal} demonstrates graphically that the Z dynamics obtained through local linear regression do not display the local shifts away from the mean seen in the Z dynamics obtained via global linear regression. 

To complete the reconstruction process, the infected dynamics are obtained by multiplying $\rho_{t}$ and by the reported cases. 

\newpage
#References
[@KeelingBook]
[@tycho]
[@SRgrenfell]