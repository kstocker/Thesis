---
title: 'Measles Data: Multi-City'
author: "Kaitlyn Stocker"
date: "3/30/2017"
output: html_document
---

```{r Libraries, include=FALSE}
library(dplyr)
library(rstan)
library(locfit)
```

```{r Functions, include=FALSE}
Sim <- function(beta, pop, i.now, s.now, r.now, births) {

results <- as.data.frame(matrix(c(i.now, s.now, r.now), nrow=1))
names(results) <- c("I", "S", "R")
  for(i in 1:545){
    if(s.now*i.now >0){
    n.now <- pop[i]
    i.next <- beta[i]*(i.now^.975)*s.now/n.now
    #i.next <- rnbinom(n=1, mu=beta[i]*(i.now^.975)*s.now/n.now, size=i.now) 
    b.now <- births[i]
    s.now <- s.now - i.next + b.now
    r.now <- r.now + i.now
    i.now <- i.next
    #cases <- i.now/rho[i]
    results <- rbind(results, c(i.now, s.now, r.now))
    }
    else {break}
  }
return(results)
}

gaussian_kernel <- function(x) {
  (1/sqrt(2*pi))*exp(-x^2/2)
}

gaussian_sum <- function(i, m) {
  gaussian_sum <- 0
  for(k in 1:m){
      gaussian_sum <- sum(gaussian_kernel((x.n[i,k]-x0[i])/which.diff[i]))
      return(gaussian_sum)
    }
}

gaussian_regression <- function(city, scale){
  cumbirths <- cumsum(city$rec)
  cumcases <- as.numeric(cumsum(city$cases))
  
  ord <- order(cumcases)
  c.cases <- cumcases[ord] 
  c.births <- cumbirths[ord] 

  m <- floor(length(cumcases)*(scale)) #SCALE GOES HERE
  h <- m/2

  x0<-0
  diffs<-matrix(nrow=length(cumcases), ncol=length(cumcases))
  which.diff<-0
  x.n <- matrix(nrow=length(cumcases), ncol=m)
  y.n <- matrix(nrow=length(cumcases), ncol=m)

  for(i in 1:length(cumcases)){
  x0[i] <- c.cases[i] #set focal x 
  diffs[i,] <- abs(c.cases - x0[i]) #difference between each datapoint and x0 (x value difference)
  which.diff[i] <- sort(diffs[i,])[m] #This returns the mth smallest difference between c.births and x0. 

  if(length(c.cases[diffs[i,] <= which.diff[i]])<m){
    x.n[i,] <- c(c.cases[diffs[i,] <= which.diff[i]], rep(0, length.out=(m-length(c.cases[diffs[i,] <= which.diff[i]])))) #select m closest x values to focal x
  y.n[i,] <- c(c.births[diffs[i,] <= which.diff[i]], 0) #select m closest corresponding y valeus 
  }
  else{
  x.n[i,] <- head(c.cases[diffs[i,] <= which.diff[i]], m) #select m closest x values to focal x
  y.n[i,] <- head(c.births[diffs[i,] <= which.diff[i]], m) #select m closest corresponding y valeus 
  }
  }
  

gaussian_kernel <- function(x) {
  (1/sqrt(2*pi))*exp(-x^2/2)
}

gaussian_sum <- function(i, m) {
  gaussian_sum <- 0
  for(k in 1:m){
      gaussian_sum <- sum(gaussian_kernel((x.n[i,k]-x0[i])/which.diff[i]))
      return(gaussian_sum)
    }
}
  
  x.wt.gaus <- matrix(nrow=length(cumcases), ncol=m)
  x.coeffs.gaus <- 0
  for(i in 1:length(cumcases)){
    for(k in 1:m){
  x.wt.gaus[i,k]<- gaussian_kernel(x=(x.n[i,k] - x0[i])/which.diff[i])/gaussian_sum(i,m)
  }
  }
  
  reg.gaus <- lapply(1:length(cumcases), function(x) lm(y.n[x,]~x.n[x,], weights = x.wt.gaus[x,]))

  x.coeffs.gaus <-0
  x.intercept.gaus <-0
  y.fit.gaus <- 0
  for(i in 1:length(reg.gaus)){
    x.coeffs.gaus[i] <- as.numeric(coef(reg.gaus[[i]])[2])
    x.intercept.gaus[i] <- as.numeric(coef(reg.gaus[[i]])[1])
    y.fit.gaus[i] <- x.coeffs.gaus[i]*c.cases[i]+x.intercept.gaus[i]
  }

T <- seq(1, 546, 1)  
x.coeffs.gaus.smooth <- fitted(loess(x.coeffs.gaus ~ T))
y.fit.gaus.smooth <-0
for(i in 1:length(x.coeffs.gaus.smooth)){
  y.fit.gaus.smooth[i] <- x.coeffs.gaus.smooth[i]*c.cases[i] + x.intercept.gaus[i]
}

return(list(y.fit.gaus=y.fit.gaus, y.fit.gaus.smooth=y.fit.gaus.smooth, x.coeffs.gaus=x.coeffs.gaus, x.coeffs.gaus.smooth=x.coeffs.gaus.smooth))
}

y_hat_linear <- function(x,y){
  as.numeric(fitted(lm(y~x)))
}

SSE <- function(y_hat, y){
  SSE<-0
  for(i in seq_along(y)){
  SSE <- sum((y[i]- y_hat[i])^2)
  return(SSE)
  }
}

linear_reconstruction <- function(city){
  cumbirths <- cumsum(city$rec)
  cumcases <- as.numeric(cumsum(city$cases))

  regression_linear <- lm(cumbirths ~ cumcases) 
  Z_linear <- as.vector(resid(regression_linear))
  
  return(list(cumbirths, cumcases, Z_linear))
}

local_regression <- function(city, scale){
  cumbirths <- cumsum(city$rec)
  cumcases <- as.numeric(cumsum(city$cases))
  
  gaus_regression_S <- gaussian_regression(city=city, scale=scale)
  gaus_regression_I <- gaussian_regression(city=city, scale=scale)

  city$Z_gaus <- cumbirths - gaus_regression_S$y.fit.gaus
  city$I_gaus <- city$cases*gaus_regression_I$x.coeffs.gaus
  
  city$Z_linear <- linear_reconstruction(city)[[3]]
  city$rho <- gaus_regression_I$x.coeffs.gaus
  
  return(city)
}

SR_plot <- function(city){
    ggplot(city, aes(x=number))+
              geom_line(aes(y=Z_gaus, col="Local Regression Estimate of Z")) +
              geom_line(aes(y=Z_linear, col="Linear Regression Estimate of Z"))+
              labs(x="Biweek", y="Z", title="Comparing Reconstruction Methods")

}


IR_plot <- function(city){
  ggplot(city, aes(x=number)) +
              geom_line(aes(y=cases, col="Reported Cases")) +
              geom_line(aes(y=I_gaus, col="Estimate of true infecteds"))+
              labs(x="Biweek", y="Infecteds/Cases", title="Reconstructed Infecteds vs Reported 
                   Cases")
}

rho_plot <- function(city){
  ggplot(city, aes(x=number)) +
                geom_line(aes(y=rho, col="rho"))+
                labs(x="biweek", y="estimated rho", title="Time Varying Reporting Rate Plot")
}

optim_bandwidth <- function(city){
  cumbirths <- cumsum(city$rec)
  cumcases <- as.numeric(cumsum(city$cases))
  
  h<- seq(0.05, 0.89, 0.01)
  SSE1_gaus <- c()
  for(i in seq_along(h)){
    if(SSE(y=cumbirths, y_hat=gaussian_regression(city=boston, scale=h[i])[[1]]))
    SSE1_gaus[i] <- SSE(y=cumbirths, y_hat=gaussian_regression(city=boston, scale=h[i])[[1]])
  }

  SSE2_gaus <- c()
  for(i in seq_along(h)){
    SSE2_gaus[i] <- SSE(y=y_hat_linear(x=cumcases, y=cumbirths), 
                        y_hat=gaussian_regression(city=city, scale=h[i])[[1]])
  }

  SSE_optim_gaus <- which.min(abs(SSE1_gaus-SSE2_gaus)) #Find where SSE1 and SSE2 intersect

  #Select Optimum scale value
  scale_optim_gaus <- h[SSE_optim_gaus]

  return(scale_optim_gaus)
}

bayes_data <- function(city){
  return(list(N=length(city$number), P=as.integer(city$pop), I=as.integer(city$I_gaus),
              Z=as.integer(city$Z_gaus), Zmin=as.integer(abs(min(city$Z_gaus)))))
}

run_bayes <- function(city){
  data <- list(N=length(city$number), P=as.integer(city$pop), I=as.integer(city$I_gaus),
              Z=as.integer(city$Z_gaus), Zmin=as.integer(abs(min(city$Z_gaus))))
  
Stan_z_mod <- as.character("
data {
	int<lower=1> N; // time steps
  real P[N]; //population size
	int <lower=0> I[N]; // infecteds
	int  Z[N]; // Zt
}

parameters {
  real <lower=P[1]/1000, upper= P[N]> Sbar;
	real<lower=0, upper=200> beta[26];
}

model {
	//priors
  Sbar ~ normal(P[1]/3, P[1]/10);
	beta[26] ~ gamma(10,4);

	//likelihood
	for (n in 2:N){
  I[n] ~ neg_binomial_2(beta[n%26+1]*(I[n-1]^.975)*(Z[n-1]+Sbar)/P[n-1], I[n-1]);
  }
}    ") 


Stan_z_mod_low_pop <- as.character("
data {
	int<lower=1> N; // time steps
  real P[N]; //population size
	int <lower=0> I[N]; // infecteds
	int  Z[N]; // Zt
  int <lower=0> Zmin;
}

parameters {
  real <lower=Zmin, upper= P[N]> Sbar;
	real<lower=0, upper=200> beta[26];
}

model {
	//priors
  Sbar ~ normal(P[1]/10, P[1]/10);
	beta[26] ~ gamma(10,4);

	//likelihood
	for (n in 2:N){
  I[n] ~ neg_binomial_2(beta[n%26+1]*(I[n-1]^.975)*(Z[n-1]+Sbar)/P[n-1], I[n-1]);
  }
}    ") 

bayes_z <- stan(model_code = Stan_z_mod_low_pop, data=data, iter=500000, chains=4, pars=c("beta", "Sbar"), include=TRUE)

return(bayes_z)
}

accuracy_check <- function(betas, bayes){
  betas_bayes <- extract(bayes, permuted=TRUE)
  beta_ <- betas_bayes$beta
  
  beta<- matrix(nrow=1000000, ncol=26)
  for(i in 2:27){
    beta[,i-1] <- beta_[,i%%26+1]
  }
  
  lowlimbeta_sr <- c()
  uplimbeta_sr <- c()
  containbeta_sr <- c()
  for(i in 1:26){lowlimbeta_sr[i] <- quantile(beta[,i], probs=.025, names=FALSE)
                uplimbeta_sr[i] <- quantile(beta[,i], probs=.975, names =FALSE)
                containbeta_sr[i]<- ifelse(lowlimbeta_sr[i] <= betas[i] & uplimbeta_sr[i] 
                                           >=betas[i], 1, 0)
  }


  beta_estimate <- c()
  for(i in 1:26){
    beta_estimate[i] <- quantile(beta[,i], probs=.5, names = FALSE)
  }

  #take difference of paper values and my estimates
  beta_resid <- beta_estimate - betas
  beta_percent_error <- beta_resid/betas

return(list(containbeta_sr=containbeta_sr, beta_resid=summary(beta_resid), beta_percent_error=summary(beta_percent_error)))
}
```



```{r Import Data, echo=FALSE}
#Read in Measles Data from Tycho Paper 
measles <- read.csv(file = "/Users/Kaitlyn/Documents/Thesis Work /Datasets/measlesUKUS.csv", header=TRUE, stringsAsFactors = FALSE)

measles_betas <- read.csv(file = "/Users/Kaitlyn/Documents/Thesis Work /Datasets/measeles biweekly betas.csv", header=TRUE, stringsAsFactors=FALSE)

#Select City Data 1920-1940, add index, select relevant variables 
newyork <- measles %>%
  filter(year>1919 & year <1941) %>%
  filter(loc == "NEW YORK") %>%
  select(biweek, year, cases, pop, rec) %>%
  mutate(number = seq(1, 546, 1))

boston <- measles %>%
  filter(year>1919 & year <1941) %>%
  filter(loc == "BOSTON") %>%
  select(biweek, year, cases, pop, rec) %>%
  mutate(number = seq(1, 546, 1))

#Pull in Paper Beta Results
ny_betas <- measles_betas %>%
  filter(city == "NEW YORK") %>%
  select(contains("biweek"))

ny_betas <- unname(unlist(ny_betas[1,]))

boston_betas <- measles_betas %>%
  filter(city == "BOSTON") %>%
  select(contains("biweek"))

boston_betas <- unname(unlist(boston_betas[1,]))

```

```{r Susceptible Reconstruction, echo=FALSE}
#Boston Reconstruction 
scale_optim_boston <- optim_bandwidth(boston)

boston <- local_regression(boston, scale_optim_boston)

SR_plot(boston)
IR_plot(boston)
rho_plot(boston)

#New York Reconstruction (12.52898 mins run time)
scale_optim_ny <- optim_bandwidth(newyork)

newyork <- local_regression(newyork, scale_optim_ny)

SR_plot(newyork)
IR_plot(newyork)
rho_plot(newyork)

bayes_data_ny <- bayes_data(newyork)
ny_bayes <- run_bayes(bayes_data_ny)
```

```{r Bayes Inference, include=FALSE}
boston_data <- bayes_data(city=boston)

boston_bayes <- run_bayes(boston)

bayes_z_boston <- stan(model_code = Stan_z_mod_low_pop, data=boston_data, iter=500000, chains=4, pars=c("beta", "Sbar"), include=TRUE)
```

```{r Check Accuracy, echo=FALSE}
boston_accuracy <- accuracy_check(betas=boston_betas, bayes=boston_bayes)

print(boston_accuracy)
```

```{r}
#Test for lowest and highest populations
measles_<- measles %>%
  filter(year==1920, biweek==1)%>%
  select(loc, pop)
  
pop <- as.vector(measles_$pop)
names(pop) <- as.vector(measles_$loc)

pop<-sort(pop)
head(pop)
tail(pop)


#Lowest Pops (ascending order):  
#DULUTH       SPOKANE     READING.US SALT LAKE CITY      NASHVILLE        TRENTON 
#98917         104437         107784         118110         118342         119289

#Highest Pops (ascending order): 
#ST LOUIS    CLEVELAND      DETROIT PHILADELPHIA      CHICAGO     NEW YORK 
#772897       796841       993078      1823779      2701705      5620048 
```

```{r}
#Function for reading in all cities:
read_in_city <- function(city){

  df <- measles %>%
  filter(year>1919 & year <1941) %>%
  filter(loc == city) %>%
  select(biweek, year, cases, pop, rec) %>%
  mutate(number = seq(1, 546, 1))
  
  betas <- measles_betas %>%
  filter(city == city) %>%
  select(contains("biweek"))
  
  betas <- unname(unlist(betas[1,]))
  
  return(list(df=df, betas=betas))
}

chicago <- read_in_city("CHICAGO")[[1]]
chicago_betas <- read_in_city("CHICAGO")[[2]]
```

To do (4/4/17):
- test modulo in simulation and stan code
- make and test general function to run all cities in one go
  - make a list of all city data so you can generalize pulling in and calling cities
- write one page summary of my SR with seasonal rho's 
- undo smoothing on rho 


```{r}
#meeting notes 3/30
# I have their betas, I know what model - don't know what rho's they fit 
# can I reverse engineer to figure out rho's?? 
#one page summary I can write up on local regression to get rho's (Dr haran can ask Matt Ferrari) -- in particular, roughly speaking 1 pg, methodology used in order to get the rho's I get. Add some lil graphs. SEND WITHIN A WEEK 

#first run analysis through all of the cities
#then talk about what's happening - what's similar, what's different... 
#what's interesting across these cities? 
#how to model stuff across cities? (really clever -not spatial model.. things that might be happening synchronicely or a-synchronicely)

#Pick 3 places and try to learn stuff across those 3 places (northeast?) - what have they written about these places - also look at places that are smaller than the big guys (at least 2 of the smaller guys). Compare across... how well things fit for the certain city... ?Proportion of 0 inferred infecteds?...are there die-outs? 

#Look at paper: jandarov (on dr haran's website) on the gravity TSIR model. Get a taste of spatial version. Cool thing: 4 signitures (signiture=characteristic of how an infectious disease plays otu) - one is the propensity to get snuffed out. THey wrote a model that reproduced those characteristics well. *does my fitted model have the same signitures as the model itself 

#forward spatial model is possible - but the inference might be too far. May even be able to do the gravity model 
```

